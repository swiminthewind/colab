{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swiminthewind/colab/blob/main/230920-210802%2B210728%E5%90%8C%E5%B9%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oojERsPi__RW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "klGbFpT8BQxi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks1/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adnALrhNBVNt"
      },
      "source": [
        "# 导入库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "JdIOgX4GBTID"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "from osgeo import gdal\n",
        "import skimage\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import keras.backend as K\n",
        "\n",
        "from skimage.transform import resize\n",
        "from keras import Model\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, Lambda, Dropout, BatchNormalization, Add, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from keras.layers import  Activation, Reshape, Dense, Flatten, ZeroPadding2D\n",
        "from keras.models import save_model, load_model\n",
        "from keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qmry7rbqHG-"
      },
      "source": [
        "# 加载影像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "CntGVua1qFrr"
      },
      "outputs": [],
      "source": [
        "def read_tiff(tiff_file):\n",
        "    data = gdal.Open(tiff_file).ReadAsArray()\n",
        "    return data\n",
        "\n",
        "def load_sentinel_data(path):\n",
        "    img_paths = sorted(glob.glob(path + '*.tif'))\n",
        "    image = [np.expand_dims(read_tiff(img).astype('float32'), -1) for img in img_paths]\n",
        "    image = np.concatenate(image, axis=-1)\n",
        "    print(\"Image shape: \", image.shape, \" Min value: \", image.min(), \" Max value: \", image.max())\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po8jwx8kqIg5"
      },
      "outputs": [],
      "source": [
        "S2_img_1 = load_sentinel_data('data/20210802_S2A_10m/')\n",
        "S2_img_2 = load_sentinel_data('data/20210728_S2A_10m/')\n",
        "S1_img_1 = load_sentinel_data('data/20210802_S1A_10m_ASCENDING/')\n",
        "S1_img_2 = load_sentinel_data('data/20210728_S1A_10m_DESCENDING/')\n",
        "S2_QA_1 = load_sentinel_data('QA/20210802_S2A_QA60/')\n",
        "S2_QA_2 = load_sentinel_data('QA/20210728_S2A_QA60/')\n",
        "roi_mask_1 = load_sentinel_data('mask/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S2_img = np.concatenate((S2_img_1, S2_img_2), axis=0)\n",
        "S1_img = np.concatenate((S1_img_1, S1_img_2), axis=0)\n",
        "S2_QA = np.concatenate((S2_QA_1, S2_QA_2), axis=0)\n",
        "roi_mask = np.concatenate((roi_mask_1, roi_mask_1), axis=0)\n",
        "S2_img.shape,S1_img.shape,S2_QA.shape,roi_mask.shape"
      ],
      "metadata": {
        "id": "L13J6B1ThHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bxIuIJgqLbT"
      },
      "source": [
        "# 切割成256*256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "9LutsbnzqKuy"
      },
      "outputs": [],
      "source": [
        "def create_idx_image(rows, cols):\n",
        "    im_idx = np.arange(rows * cols).reshape(rows, cols)\n",
        "    return im_idx\n",
        "\n",
        "def extract_patches(im_idx, patch_size, overlap):\n",
        "    '''overlap range: 0 - 1 '''\n",
        "    row_steps, cols_steps = int((1-overlap) * patch_size[0]), int((1-overlap) * patch_size[1])\n",
        "    patches = skimage.util.view_as_windows(im_idx, patch_size, step=(row_steps, cols_steps))\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "5pdx9Wsm-Kbs"
      },
      "outputs": [],
      "source": [
        "rows, cols = S2_img.shape[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "h2NqEg__bm_m"
      },
      "outputs": [],
      "source": [
        "im_idx = create_idx_image(rows, cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "0AwYzgILbn6L"
      },
      "outputs": [],
      "source": [
        "patches_idx = extract_patches(im_idx, patch_size=(256, 256), overlap=0).reshape(-1, 256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGua303XAyl8"
      },
      "outputs": [],
      "source": [
        "patches_idx.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJNjLzXbAuQw"
      },
      "source": [
        "# 划分clouds_tiles和no_clouds_tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "FnqOjwDIBGph"
      },
      "outputs": [],
      "source": [
        "c_dim_QA = S2_QA.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "5yRxsn-uDmiO"
      },
      "outputs": [],
      "source": [
        "def check_clouds_in_array(arr):\n",
        "    flat_arr = arr.flatten()\n",
        "    if 1024 in flat_arr or 2048 in flat_arr:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "o7KmLhr5EERc"
      },
      "outputs": [],
      "source": [
        "no_clouds_tiles = []\n",
        "clouds_tiles = []\n",
        "for i in range(len(patches_idx)):\n",
        "    if check_clouds_in_array(S2_QA.reshape(-1, c_dim_QA)[patches_idx[i]]) == False:\n",
        "        no_clouds_tiles.append(i)\n",
        "    else:\n",
        "        clouds_tiles.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe50CNSEFLXa"
      },
      "outputs": [],
      "source": [
        "print(clouds_tiles)\n",
        "print(no_clouds_tiles)\n",
        "print(len(clouds_tiles))\n",
        "print(len(no_clouds_tiles))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNrN0x_lEmZ9"
      },
      "source": [
        "# 划分roi_tiles和no_roi_tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "klZZw0qwEpv-"
      },
      "outputs": [],
      "source": [
        "c_dim_mask = roi_mask.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "5e3Y7fAuE8Eo"
      },
      "outputs": [],
      "source": [
        "def check_bounds_in_array(arr):\n",
        "    flat_arr = arr.flatten()\n",
        "    if 1 in flat_arr:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "EiQ93w8ZFH_0"
      },
      "outputs": [],
      "source": [
        "roi_tiles = []\n",
        "no_roi_tiles = []\n",
        "for i in range(len(patches_idx)):\n",
        "    if check_bounds_in_array(roi_mask.reshape(-1, c_dim_mask)[patches_idx[i]]) == True:\n",
        "        roi_tiles.append(i)\n",
        "    else:\n",
        "        no_roi_tiles.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2eX3FIOFVhG"
      },
      "outputs": [],
      "source": [
        "print(roi_tiles)\n",
        "print(no_roi_tiles)\n",
        "print(len(roi_tiles))\n",
        "print(len(no_roi_tiles))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUyph6P6ZW0E"
      },
      "source": [
        "# Split image into training, validation and testing tiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "uvdLsyJOZpvj"
      },
      "outputs": [],
      "source": [
        "def common_elements(a, b):\n",
        "    set_a = set(a)\n",
        "    set_b = set(b)\n",
        "    common_set = set_a.intersection(set_b)\n",
        "    return list(common_set)\n",
        "\n",
        "tiles_idx = common_elements(roi_tiles, no_clouds_tiles)\n",
        "tst_tiles_idx = common_elements(roi_tiles, clouds_tiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKtXg709eQb3"
      },
      "outputs": [],
      "source": [
        "print(tst_tiles_idx)\n",
        "print(len(tst_tiles_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "O_m4FIITZf0x"
      },
      "outputs": [],
      "source": [
        "def define_trn_val_tst_mask(tiles_grid_idx, tiles_idx, tst_tiles_idx, no_roi_tiles, grid_size=(10,10), plot=True):\n",
        "    num_tiles_rows = grid_size[0]\n",
        "    num_tiles_cols = grid_size[1]\n",
        "\n",
        "    tiles_idx = np.random.permutation(tiles_idx)\n",
        "    trn_tiles_idx = tiles_idx[:int(0.7*len(tiles_idx))]\n",
        "    val_tiles_idx = tiles_idx[int(0.7*len(tiles_idx)):]\n",
        "\n",
        "    print(trn_tiles_idx, val_tiles_idx)\n",
        "    print(len(trn_tiles_idx),len(val_tiles_idx))\n",
        "\n",
        "    tiles_numbers = np.zeros_like(tiles_grid_idx)\n",
        "    for i in range(len(tiles_grid_idx)):\n",
        "        tiles_numbers[i] = i\n",
        "\n",
        "    mask = np.zeros_like(tiles_grid_idx)\n",
        "    for idx in val_tiles_idx:\n",
        "        mask[tiles_numbers==idx] = 1\n",
        "    for idx in tst_tiles_idx:\n",
        "        mask[tiles_numbers==idx] = 2\n",
        "    for idx in no_roi_tiles:\n",
        "        mask[tiles_numbers==idx] = 3\n",
        "\n",
        "    mask = mask_tiles(mask.reshape(num_tiles_rows, num_tiles_cols, 256, 256))\n",
        "    if plot:\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(mask, cmap='PuBuGn')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "    return mask\n",
        "\n",
        "def mask_tiles(patches):\n",
        "    num_blocks_r, num_blocks_c, rows_block, cols_block = patches.shape\n",
        "    img = np.zeros((num_blocks_r*rows_block, num_blocks_c*cols_block), dtype=patches.dtype)\n",
        "    for i in range(num_blocks_r):\n",
        "        for j in range(num_blocks_c):\n",
        "            img[rows_block*i:(rows_block*i+rows_block), cols_block*j:(cols_block*j+cols_block)] = patches[i,j]\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY6ohjTXh2Vn"
      },
      "outputs": [],
      "source": [
        "mask_trn_val_tst = define_trn_val_tst_mask(patches_idx, tiles_idx, tst_tiles_idx, no_roi_tiles, (rows//256, cols//256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10bH7p6ICPS"
      },
      "source": [
        "# Extract patches from the training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "opJwtlcTICh7"
      },
      "outputs": [],
      "source": [
        "overlap=0\n",
        "patches_mask = extract_patches(mask_trn_val_tst, patch_size=(256, 256), overlap=overlap).reshape(-1, 256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_WmKoNPILlQ"
      },
      "outputs": [],
      "source": [
        "idx_trn = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==0))\n",
        "idx_val = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==256**2))\n",
        "idx_tst = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==2*256**2))\n",
        "\n",
        "patches_idx_trn = patches_idx[idx_trn]\n",
        "patches_idx_val = patches_idx[idx_val]\n",
        "patches_idx_tst = patches_idx[idx_tst]\n",
        "\n",
        "print('Number of training and validation patches: ', len(idx_trn), len(idx_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwc0ydrOI5GS"
      },
      "source": [
        "# 影像处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "tLT2JaSvI4Pq"
      },
      "outputs": [],
      "source": [
        "def db2intensities(img):\n",
        "    img = 10**(img/10.0)\n",
        "    return img\n",
        "\n",
        "def filter_outliers(img, bins=1000000, bth=0.03, uth=0.97, mask=[0]):\n",
        "    img[np.isnan(img)]=0 # Filter NaN values.\n",
        "    if len(mask)==1:\n",
        "        mask = np.zeros((img.shape[:2]), dtype='int64')\n",
        "    for band in range(img.shape[-1]):\n",
        "        hist = np.histogram(img[:mask.shape[0], :mask.shape[1]][mask!=2, band].ravel(),bins=bins) # select not testing pixels\n",
        "        cum_hist = np.cumsum(hist[0])/hist[0].sum()\n",
        "        max_value = np.ceil(100*hist[1][len(cum_hist[cum_hist<uth])])/100\n",
        "        min_value = np.ceil(100*hist[1][len(cum_hist[cum_hist<bth])])/100\n",
        "        img[:,:, band][img[:,:, band]>max_value] = max_value\n",
        "        img[:,:, band][img[:,:, band]<min_value] = min_value\n",
        "    return img\n",
        "\n",
        "def normalize(img):\n",
        "    '''image shape: [row, cols, channels]'''\n",
        "    img = 2*(img -img.min(axis=(0,1), keepdims=True))/(img.max(axis=(0,1), keepdims=True) - img.min(axis=(0,1), keepdims=True)) - 1\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eebjR6_kJB6G"
      },
      "outputs": [],
      "source": [
        "S1_img = db2intensities(S1_img)\n",
        "S1_img.min(), S1_img.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_d5b568JDNm"
      },
      "outputs": [],
      "source": [
        "S1_img = filter_outliers(S1_img.copy())\n",
        "S2_img = filter_outliers(S2_img.copy(), bins=int(2**16/2), mask=mask_trn_val_tst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTRBY2XmJDp8"
      },
      "outputs": [],
      "source": [
        "S1_img = normalize(S1_img)\n",
        "S2_img = normalize(S2_img)\n",
        "S1_img.min(), S1_img.max(), S2_img.min(), S2_img.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A89T34tdH7Ka"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow((S2_img[:,:,[2,1,0]] + 1)/2)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7tJabEEKa82"
      },
      "source": [
        "# 影像切片"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUD5WKGYJvbI"
      },
      "outputs": [],
      "source": [
        "# c_dim_opt = S2_img.shape[-1]\n",
        "# S2_img_1 = S2_img.reshape(-1, c_dim_opt)[patches_idx[27]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RASZEGH1JyTL"
      },
      "outputs": [],
      "source": [
        "# print(S2_img_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMHsF5_gJ1F7"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10,10))\n",
        "# plt.imshow((S2_img_1[:,:,[2,1,0]] + 1)/2)\n",
        "# plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hkPOQ8ceVeT"
      },
      "source": [
        "# cGANs训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U67yXskYeVES"
      },
      "outputs": [],
      "source": [
        "c_dim_sar = S1_img.shape[-1]\n",
        "c_dim_opt = S2_img.shape[-1]\n",
        "n_rows = 256\n",
        "n_cols = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cxukD2teZiM"
      },
      "source": [
        "## 数据增强\n",
        "Resize the paches to bigger height and width.\n",
        "\n",
        "1. Resize the paches to bigger height and width.\n",
        "2. Randomly crop to the target size.\n",
        "3. Randomly flip the image horizontally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IEecmHFeY9u"
      },
      "outputs": [],
      "source": [
        "def randomResizeCrop(img_s1, img_s2, load_size=286, fine_size=256, flip=True):\n",
        "\n",
        "    img_s1 = resize(img_s1, [load_size, load_size], 0)\n",
        "    img_s2 = resize(img_s2, [load_size, load_size], 0)\n",
        "\n",
        "    h1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
        "    w1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
        "    img_s1 = img_s1[h1:h1+fine_size, w1:w1+fine_size]\n",
        "    img_s2 = img_s2[h1:h1+fine_size, w1:w1+fine_size]\n",
        "\n",
        "    if flip and np.random.random() > 0.5:\n",
        "        img_s1 = np.fliplr(img_s1)\n",
        "        img_s2 = np.fliplr(img_s2)\n",
        "\n",
        "    return img_s1, img_s2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKhLDPVkeddV"
      },
      "source": [
        "## 构建生成器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d60FoTWTecbk"
      },
      "outputs": [],
      "source": [
        "# define an encoder block\n",
        "def encoder_block(input_data, n_filters, k_size=3, strides=2, activation='relu', padding='same', batchnorm=True, name='None'):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    x = Conv2D(n_filters, k_size, strides=strides, padding=padding, kernel_initializer=init, name=name+'_conv2D')(input_data)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization(momentum=0.8, name=name+'_bn')(x, training=True)\n",
        "    if activation is 'LReLU':\n",
        "        x = LeakyReLU(alpha=0.2, name=name+'_act_LReLU')(x)\n",
        "    else:\n",
        "        x = Activation('relu', name=name+'_act_relu')(x)\n",
        "    return x\n",
        "\n",
        "# define a decoder block\n",
        "def decoder_block(input_data, n_filters, k_size=3, strides=2, padding='same', name='None'):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    x = Conv2DTranspose(n_filters, k_size, strides=strides, padding=padding, kernel_initializer=init, name=name+'_deconv2D')(input_data)\n",
        "    x = BatchNormalization(momentum=0.8, name=name+'_bn')(x, training=True)\n",
        "    x = Activation('relu', name=name+'_act_relu')(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(input_x, n_kernels, name='name'):\n",
        "    x = encoder_block(input_x, n_kernels, strides=1, name=name+'rba')\n",
        "    x = Dropout(0.5, name=name+'drop')(x, training=True)\n",
        "    x = encoder_block(x, n_kernels,  strides=1, activation='linear', name=name+'rbb')\n",
        "    x = Add(name=name+'concatenate')([x, input_x])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpJelqXieqh3"
      },
      "outputs": [],
      "source": [
        "def build_generator2D(model_shape, filters=64, last_act='tanh', n_residuals=9, summary=False, model_file=None, name='gan_g_'):\n",
        "\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    n_rows = model_shape[0]\n",
        "    n_cols = model_shape[1]\n",
        "    in_c_dims = model_shape[2]\n",
        "    out_c_dims = model_shape[3]\n",
        "\n",
        "    input_shape = (n_rows, n_cols, in_c_dims)\n",
        "    input_layer = Input(shape=input_shape, name=name+'_input')\n",
        "\n",
        "    x = input_layer\n",
        "    x = encoder_block(x, 1*filters, k_size=7, strides=1, batchnorm=False, name=name+'_e1')\n",
        "    x = encoder_block(x, 2*filters, name=name+'e2') # rows/2, cols/2\n",
        "    x = encoder_block(x, 4*filters, name=name+'e3') # rows/4, cols/4\n",
        "\n",
        "    for i in range(n_residuals):\n",
        "        x = residual_block(x, n_kernels=4*filters, name=name+str(i+1)+'_')  # rows/4, cols/4\n",
        "\n",
        "    x = decoder_block(x, 2*filters, name=name+'d1') # rows/2, cols/2\n",
        "    x = decoder_block(x, 1*filters, name=name+'d2') # rows, cols\n",
        "    x = Conv2D(out_c_dims, 7, padding='same',  kernel_initializer=init, name=name+'d_out')(x)   # rows, cols\n",
        "\n",
        "    output = Activation(last_act, name=name+last_act)(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='Generator'+name[-3:])\n",
        "    if (summary):\n",
        "        model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1uxKWaDetP-"
      },
      "outputs": [],
      "source": [
        "# Setting the model shape\n",
        "gen_shape = (n_rows, n_cols, c_dim_sar, c_dim_opt)\n",
        "generator = build_generator2D(gen_shape, filters=64, name='gen', summary=True)\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbUQ0ACte6-v"
      },
      "source": [
        "## 构建判别器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx_JM_Oye9fi"
      },
      "outputs": [],
      "source": [
        "########################  NETWORK ARCHITECTURES  ###############################\n",
        "def build_discriminator2D(input_shape, filters=64, name='d', summary=False):\n",
        "    \"\"\"\n",
        "    Create a Discriminator Model using hyperparameters values defined as follows\n",
        "    \"\"\"\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "\n",
        "    input_img  = Input(shape=input_shape, name=name+'_input')\n",
        "    d = input_img\n",
        "    d = encoder_block(d, 1*filters, k_size=4, activation='LReLU', batchnorm=False, name=name+'_1')\n",
        "    d = encoder_block(d, 2*filters, k_size=4, activation='LReLU', name=name+'_2')\n",
        "    d = encoder_block(d, 4*filters, k_size=4, activation='LReLU', name=name+'_3')\n",
        "\n",
        "    d = ZeroPadding2D()(d)\n",
        "    d = encoder_block(d, 8*filters, k_size=4, activation='LReLU', strides=1, padding='valid', name=name+'_4')\n",
        "    d = ZeroPadding2D()(d)\n",
        "    logits = Conv2D(1, (4,4), padding='valid', kernel_initializer=init, name=name+'_conv2D_5')(d)\n",
        "    out = Activation('sigmoid', name=name+'_act_sigmoid')(logits)\n",
        "\n",
        "    model = Model(inputs=[input_img], outputs=[out, logits], name=name)\n",
        "    if (summary):\n",
        "        model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYetdHN9e-cd"
      },
      "outputs": [],
      "source": [
        "dis_shape = (n_rows, n_cols, c_dim_sar + c_dim_opt)\n",
        "discriminator = build_discriminator2D(dis_shape, filters=64, name='dis', summary=True)\n",
        "# tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD5SqztkfGXa"
      },
      "source": [
        "## 损失函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ulL3W2e_Y6"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(labels, logits):\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "    return loss\n",
        "\n",
        "def lsgan_loss(labels, logits):\n",
        "        loss = tf.reduce_mean(tf.squared_difference(logits, labels))\n",
        "        return loss\n",
        "\n",
        "def l1_loss(a, b):\n",
        "    loss = tf.reduce_mean(tf.abs(a - b))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ETcyAh6fIUc"
      },
      "source": [
        "## 构建计算图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwjqIGPNfJy0"
      },
      "outputs": [],
      "source": [
        "K.clear_session() # cleaning previous sessions\n",
        "\n",
        "batch_size = 1\n",
        "LAMBDA = 100\n",
        "\n",
        "# BUILDING NETWORKS GRAPHS\n",
        "Generator = build_generator2D(gen_shape, filters=64, name='gen', summary=False)\n",
        "Discriminator = build_discriminator2D(dis_shape, filters=64, name='dis', summary=False)\n",
        "\n",
        "# GRAPH INPUT DATA\n",
        "REAL_SAR = tf.placeholder(tf.float32,\n",
        "                        [batch_size, n_rows, n_cols, c_dim_sar],\n",
        "                        name='real_sar')\n",
        "\n",
        "REAL_OPT = tf.placeholder(tf.float32,\n",
        "                        [batch_size, n_rows, n_cols, c_dim_opt],\n",
        "                        name='real_opt')\n",
        "\n",
        "FAKE_OPT = Generator(REAL_SAR)\n",
        "\n",
        "REAL_PAIR = tf.concat([REAL_SAR, REAL_OPT], axis=-1)\n",
        "FAKE_PAIR = tf.concat([REAL_SAR, FAKE_OPT], axis=-1)\n",
        "\n",
        "# Discriminator\n",
        "D_real, D_real_logits = Discriminator(REAL_PAIR)\n",
        "D_fake, D_fake_logits = Discriminator(FAKE_PAIR)\n",
        "\n",
        "# Discriminator loss\n",
        "d_loss_real = cross_entropy_loss(tf.ones_like(D_real), D_real_logits)\n",
        "d_loss_fake = cross_entropy_loss(tf.zeros_like(D_real), D_fake_logits)\n",
        "d_loss = (d_loss_real + d_loss_fake) / 2.0\n",
        "\n",
        "# Reconstruction loss\n",
        "reco_loss = LAMBDA * l1_loss(REAL_OPT, FAKE_OPT)\n",
        "\n",
        "# Generator loss\n",
        "g_loss_ = cross_entropy_loss(tf.ones_like(D_fake), D_fake_logits)\n",
        "g_loss =  g_loss_ + reco_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3VV4lcxfQE3"
      },
      "source": [
        "### 定义优化器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6Wcrd80fRbf"
      },
      "outputs": [],
      "source": [
        "# Collecting variables for training\n",
        "t_vars = tf.trainable_variables()\n",
        "# print(t_vars)\n",
        "\n",
        "d_vars = [var for var in t_vars if 'dis' in var.name] # Discriminator variables\n",
        "g_vars = [var for var in t_vars if 'gen' in var.name] # Generator variables\n",
        "\n",
        "# Optimizer parameters\n",
        "lr_d = 0.0002\n",
        "lr_g = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Assings variables and corresponding lossses to be minimized\n",
        "d_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
        "g_optim = tf.train.AdamOptimizer(lr_g, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "# Build the graph\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initialize the all variables\n",
        "init_op = tf.global_variables_initializer()\n",
        "sess.run(init_op)\n",
        "\n",
        "# Add ops to save and restore all the variables.\n",
        "saver = tf.train.Saver()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG_sB5CLfTnV"
      },
      "source": [
        "### 设置超参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16Omg2dkfUgT"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "patience = 15 # for early stoppig\n",
        "num_of_trn_batches = len(patches_idx_trn) // batch_size\n",
        "num_of_val_batches = len(patches_idx_val) // batch_size\n",
        "# print(num_of_trn_batches)\n",
        "# print(num_of_val_batches)\n",
        "\n",
        "g_steps = 1\n",
        "\n",
        "saving_path = './'\n",
        "\n",
        "name = 'sar2opt'\n",
        "\n",
        "restore = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR29kV-rfWqV"
      },
      "outputs": [],
      "source": [
        "def get_next_batch_generator(patches_idx, s1data, s2data, batch_size=1, shuffle=True, flip=True, train=True):\n",
        "    num_of_batches = len(patches_idx) // batch_size\n",
        "    s1data = s1data.reshape(-1, s1data.shape[-1])\n",
        "    s2data = s2data.reshape(-1, s2data.shape[-1])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(patches_idx)\n",
        "    while True:\n",
        "        for idx in range(num_of_batches):\n",
        "            patches_idx_batch = patches_idx[idx*batch_size:(idx+1)*batch_size]\n",
        "            batch_s1_patches, batch_s2_patches = [], []\n",
        "            for batch_idx in patches_idx_batch:\n",
        "                batch_s1, batch_s2 = s1data[batch_idx], s2data[batch_idx]\n",
        "                if train:\n",
        "                    # data augmentation\n",
        "                    batch_s1, batch_s2 = randomResizeCrop(batch_s1, batch_s2, flip=flip)\n",
        "                batch_s1_patches.append(batch_s1)\n",
        "                batch_s2_patches.append(batch_s2)\n",
        "\n",
        "            yield np.array(batch_s1_patches), np.array(batch_s2_patches)\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(patches_idx)\n",
        "        idx = 0\n",
        "\n",
        "def plot_images(sar, real_opt, fake_opt, figsize=(10, 5)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    ax1 = fig.add_subplot(131)\n",
        "    plt.title('SAR Image')\n",
        "    ax1.imshow((np.squeeze(sar)[:,:,0]+1)/2., cmap='gray')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    plt.title('Target')\n",
        "    ax2.imshow((np.squeeze(real_opt)[:,:,:3]+1)/2.)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    ax2 = fig.add_subplot(133)\n",
        "    plt.title('Predicted')\n",
        "    ax2.imshow((np.squeeze(fake_opt)[:,:,:3]+1)/2.)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    # fig.savefig(save_img_path+name+'_img_pt_br_2_elastic_'+str(epoch))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4JhKJ75fXuE"
      },
      "outputs": [],
      "source": [
        "get_next_trn_batch = get_next_batch_generator(patches_idx_trn, S1_img, S2_img, flip=True)\n",
        "get_next_val_batch = get_next_batch_generator(patches_idx_val, S1_img, S2_img, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4q3IW9ifZz_"
      },
      "source": [
        "## 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6b5THtfgfkl"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "\n",
        "    if restore:\n",
        "        # Restore variables from disk.\n",
        "        saver.restore(sess, saving_path+name +\".ckpt\")\n",
        "        print(\"Model restored.\")\n",
        "\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    for epoch in range(epochs):\n",
        "        loss_D, loss_G, loss_l1 = [], [], []\n",
        "\n",
        "        start_time = time.time()\n",
        "        for idx in range(0, num_of_trn_batches):\n",
        "\n",
        "            # selecting a batch of images\n",
        "            batch_sar, batch_opt = next(get_next_trn_batch)\n",
        "\n",
        "            feed_dict={REAL_SAR: batch_sar, REAL_OPT: batch_opt}\n",
        "\n",
        "            # Update D networks\n",
        "            sess.run([d_optim], feed_dict=feed_dict)\n",
        "\n",
        "            # Update G network\n",
        "            for g_ in range(g_steps):\n",
        "                sess.run([g_optim], feed_dict=feed_dict)\n",
        "\n",
        "            with sess.as_default():\n",
        "                errD = d_loss.eval(feed_dict)\n",
        "                errG = g_loss_.eval(feed_dict)\n",
        "                errl1 = reco_loss.eval(feed_dict)\n",
        "                loss_D.append(errD)\n",
        "                loss_G.append(errG)\n",
        "                loss_l1.append(errl1)\n",
        "\n",
        "            if idx % (num_of_trn_batches//1) == 0:\n",
        "                print('Random training samples')\n",
        "                pred = Generator.predict(batch_sar)\n",
        "                plot_images(batch_sar, batch_opt, pred)\n",
        "\n",
        "        # Evaluating model on validation,\n",
        "        val_loss = []\n",
        "        for _ in range(0, num_of_val_batches):\n",
        "            batch_sar, batch_opt = next(get_next_val_batch)\n",
        "            feed_dict={REAL_SAR: batch_sar, REAL_OPT: batch_opt}\n",
        "            val_loss.append(reco_loss.eval(feed_dict))\n",
        "\n",
        "        if best_val_loss > np.mean(val_loss):\n",
        "            patience = 15\n",
        "            best_val_loss = np.mean(val_loss)\n",
        "            print('Saving best model and checkpoints')\n",
        "            save_model(Generator, saving_path+name+'_gen_net.h5')\n",
        "            save_model(Discriminator, saving_path+name+'_dis_net.h5')\n",
        "            # Save the variables to disk.\n",
        "            saver.save(sess, saving_path+name +\".ckpt\")\n",
        "            print('Ok')\n",
        "        else:\n",
        "            patience -= 1\n",
        "        if patience < 0:\n",
        "            break\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print('Epoch: ', epoch, 'Elapsed time:', elapsed_time,  'Val_loss: ', np.mean(val_loss))\n",
        "        print('Dx_loss :', np.mean(loss_D), 'G_loss :', np.mean(loss_G), 'l1_loss :', np.mean(loss_l1))\n",
        "\n",
        "        print('Random validations samples')\n",
        "        pred = Generator.predict(batch_sar)\n",
        "\n",
        "        plot_images(batch_sar, batch_opt, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v-0t0iZgnrl"
      },
      "outputs": [],
      "source": [
        "## Cleaning the session to load best model\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAKjiDxdoqFP"
      },
      "source": [
        "# Evaluate the model on the test regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtJ2yHZpor3h"
      },
      "outputs": [],
      "source": [
        "class sar2opt_model(object):\n",
        "    def __init__(self, model_weights, input_dim=c_dim_sar):\n",
        "        self.model_weights=model_weights\n",
        "        self.input_dim=input_dim\n",
        "        self.gen_net = self.load_gen_model()\n",
        "\n",
        "    def load_gen_model(self):\n",
        "        gen_net = load_model(self.model_weights)\n",
        "        gen_net.layers.pop(0)\n",
        "        new_input = Input(shape=(None, None, self.input_dim), name='input')\n",
        "        new_output = gen_net(new_input)\n",
        "        net = Model(inputs=[new_input], outputs=[new_output], name='sar2optical')\n",
        "        return net\n",
        "\n",
        "    def predict(self, image):\n",
        "        rows, cols = image.shape[:2]\n",
        "        new_img = self.toNewImgSize(image)\n",
        "        pred = self.gen_net.predict(new_img)\n",
        "        pred = np.squeeze(pred)[:rows, :cols]\n",
        "        return pred\n",
        "\n",
        "    def toNewImgSize(self, image):\n",
        "        rows, cols = image.shape[:2]\n",
        "        new_rows = rows + 4 - rows % 4\n",
        "        new_cols = cols + 4 - cols % 4\n",
        "        NewImg = image.max() * np.ones((new_rows, new_cols, c_dim_sar))\n",
        "        NewImg[:rows, :cols] = image.copy()\n",
        "        return np.expand_dims(NewImg,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGnMchEKouid"
      },
      "outputs": [],
      "source": [
        "gen_model = sar2opt_model('sar2opt_gen_net.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkx9D9xNowwo"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tst_tiles_idx)):\n",
        "    S1_tst_tile = S1_img.reshape(-1, c_dim_sar)[patches_idx[tst_tiles_idx[i]]]\n",
        "    S2_tst_tile = S2_img.reshape(-1, c_dim_opt)[patches_idx[tst_tiles_idx[i]]]\n",
        "    pred_tst_tile = gen_model.predict(S1_tst_tile)\n",
        "    plot_images(S1_tst_tile, S2_tst_tile, pred_tst_tile, figsize=(20,10))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEt16xZUUsVLK94a1FZASj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}